---
title: "Breast Cancer Biopsy Classification Project"
subtitle:  "Guy Leib (316311190), Shir Amit Fishbein (207640228), Nadav Klein (318865698)"
authors: Guy Leib, Shir Amit Fishbein, Nadav Klein
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r imports, include=FALSE}
library(ggplot2)
library(tidyverse)
library(patchwork)
library(dplyr) 
```
```{r}
#install.packages("GGally")
#install.packages("corrplot")
library(corrplot)
library(GGally)
library(plotly)
```


<Opening paragraph>

# The Data
Biopsy features for classification of 569 malignant (cancer) and benign (not cancer) breast masses.
Features were computationally extracted from digital images of fine needle aspirate biopsy slides.
## columns description
In each observation, 10 attributes were measured for *every cell* in the biopsy (see the list below).
Our data contain summary of those results and each row in the data contain only the mean, the standard error and the worst case among the cells (can be the larger or the most severe, depending the feature).
overall, the data contains 3*10 features and a label (column Y) that mention wherever the mass is malignant (M) or benign (B).
*list of the 10 measures:*
radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension.
radius: 	Nucleus radius (mean of distances from center to points on perimeter).
texture:	 Nucleus texture (standard deviation of grayscale values).
perimeter:	Nucleus perimeter.
area:	Nucleus area.
smoothness:	Smoothness of the tumor cells.
compactness:	 Nucleus compactness (perimeter^2/area - 1).
concavity:	Nucleus concavity (severity of concave portions of the contour).
concave_points:	Number of concave portions of the nucleus contour.
symmetry:	Nucleus Symmetry.
fractal_dimension: Nucleus fractal dimension ("coastline approximation" -1).


# Get started
## Upload and initial cleaning of the data
We are first changing the names of the columns and the labels from 'M' and 'B' to 'Malignant' and 'Benign'.
```{r load, message=FALSE, warning=FALSE, echo=FALSE}
brca <-read.csv("data//brca.csv")
```
```{r cleaning}
# remove the prefix "x." from column names
colnames(brca) <- gsub("^x\\.", "", colnames(brca))
colnames(brca)[which(names(brca) == "y")] <- "diagnosis"
# remove id column
brca<-brca[,-1]
# factorize the label
brca <- brca %>% mutate(diagnosis = if_else(diagnosis == "B", "Benign", "Malignant"))
brca$diagnosis <- as.factor(brca$diagnosis)
brca.x <- brca[,!(colnames(brca) %in% c("diagnosis"))]
colnames(brca)
```
## Show values range
Afterward, to examine our data, we want to see the ranges of the values within each column
```{r examine}
measures <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave_pts", "symmetry", "fractal_dim")
cat("Range of values in the numerical columns: \n")
for (prefix in measures) {
  # Find column names that start with the current prefix
  cat("-------", prefix, "-------\n")
  cat("\t\t\t min:\t\t max:\t\t mean:\n")
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")

  # Print the result
  cat("Mean:\t\t\t", round(min(brca[[prefix.mean]]), 2), "\t\t" , round(max(brca[[prefix.mean]]), 2), "\t\t", round(mean(brca[[prefix.mean]]) ,2), "\n")
  cat("Standard error:\t\t", round(min(brca[[prefix.se]]), 2), "\t\t" , round(max(brca[[prefix.se]]), 2), "\t\t", round(mean(brca[[prefix.se]]) ,2), "\n")
  cat("Worst:\t\t\t", round(min(brca[[prefix.worst]]), 2), "\t\t" , round(max(brca[[prefix.worst]]), 2), "\t\t", round(mean(brca[[prefix.worst]]) ,2), "\n")
}
cat("\n\nnumber of NAs in dataset: ",sum(is.na(brca)))
```

Above we see the ranges of the values in our data and that there are no missing values. all the features are numeric.
# Data cleaning
## Normalization
Because we have different ranges and we don't want that the scale of the feature will be a factor during the study we would like to normalize each column to values between 0 and 1 by preforming Min-Max normalization
```{r norm}
minmax <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
brca.x.normal <- as.data.frame(lapply(brca.x, minmax))
# Retrieve the diagnosis column
diagnosis <- brca$diagnosis

# Combine normalized data frame with diagnosis column
brca.normal <- cbind(diagnosis, brca.x.normal)
```
```{r show norm range}
for (prefix in measures) {
  # Find column names that start with the current prefix
  cat("-------", prefix, "-------\n")
  cat("\t\t\t min:\t\t max:\t\t mean:\n")
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")

  # Print the result
  cat("Mean:\t\t\t", round(min(brca.x.normal[[prefix.mean]]), 2), "\t\t" , round(max(brca.x.normal[[prefix.mean]]), 2), "\t\t", round(mean(brca.x.normal[[prefix.mean]]) ,2), "\n")
  cat("Standard error:\t\t", round(min(brca.x.normal[[prefix.se]]), 2), "\t\t" , round(max(brca.x.normal[[prefix.se]]), 2), "\t\t", round(mean(brca.x.normal[[prefix.se]]) ,2), "\n")
  cat("Worst:\t\t\t", round(min(brca.x.normal[[prefix.worst]]), 2), "\t\t" , round(max(brca.x.normal[[prefix.worst]]), 2), "\t\t", round(mean(brca.x.normal[[prefix.worst]]) ,2), "\n")
}
```

# Exploratory Data Analysis
## Balance
First, we want to see if the labels are balanced:
```{r}
df <- brca["diagnosis"] %>% 
  group_by(diagnosis) %>%
  count() %>% 
  ungroup() %>% 
  mutate(perc = `n` / sum(`n`)) %>% 
  arrange(perc) %>%
  mutate(labels = scales::percent(perc))

ggplot(df, aes(x = "", y = perc, fill = diagnosis)) +
  geom_col() +
  geom_label(aes(label = labels),
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +
  coord_polar(theta = "y")
```
We can see that the data doesn't contain significant bias toward one of the labels.
it is possible that it could effect on the results but from the other hand we don't want to lose data that can be significant.
therefore, considering that the bias is not very large, we decided to not make changes.

## distributions differences
We want to explore if we can see visually how the measures are related to the diagnosis,
we create box plots to each aspect of each measure for both labels - Benign and Malignant.
```{r mean plot}
box_df <- as_tibble(brca.x.normal) %>%
  select(ends_with("_mean")) %>%
  rename_all(~ str_replace_all(., "_mean", "")) %>%
  mutate(brca["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")
p.mean <- ggplot(box_df,
       aes(factor(features,levels = measures),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c("#75c731", "#0488cf")) +
  labs(x = "Feature (mean)", y = "Standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))
p.mean + plot_annotation(title = "Mean values distribution")
```
```{r se plot}
box_df <- as_tibble(brca.x.normal) %>%
  select(ends_with("_se")) %>%
  rename_all(~ str_replace_all(., "_se", "")) %>%
  mutate(brca["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")
p.mean <- ggplot(box_df,
       aes(factor(features,levels = measures),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c("#75c731", "#0488cf")) +
  labs(x = "Feature (standard error)", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))
p.mean + plot_annotation(title = "Standard error values distribution")
```
```{r worst plot}
box_df <- as_tibble(brca.x.normal) %>%
  select(ends_with("_worst")) %>%
  rename_all(~ str_replace_all(., "_worst", "")) %>%
  mutate(brca["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")
p.mean <- ggplot(box_df,
       aes(factor(features,levels = measures),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c("#75c731", "#0488cf")) +
  labs(x = "Feature (worst)", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))
p.mean + plot_annotation(title = "Worst values distribution")
```
We see from the plots that the features are indeed related to the diagnosis, but that a single feature cannot predict the label alone.
Our challenge during this work will be to combine the features in a way that will give the best prediction.


# Feture selection
Because the nature of our data, having 3 aspects of every feature, there might be biases while applying VM models using all the features, where there can be duplicates in the trends between them.
moreover, the "worst" columns are often in correlation with the "mean" and the "se" of the same measure. [see supplement figures]
To overcome this, we apply PCA on each triplet of "mean", "se" and "worst" and create new data-frame with columns of the PCs (until 95% contribution of variables) instead of the whole triplets.
In addition, even between the different measures, we can find duplicates:
The radius of a cell has strong correlation to it's perimeter and area [also in the supplement figures].
To present our claim here is the correlations between all the features:
```{r corr before pca}
corrs <- cor(brca.x.normal)
colnames(corrs) <- gsub("_", " ", colnames(corrs))
rownames(corrs) <- gsub("_", " ", rownames(corrs))
corrplot(corrs, method = "square", tl.col = "black", tl.srt = 45, sig.level = 0.05, type = 'lower', diag = FALSE)
```
To overcome this, we:
1) preform PCA on all the aspects of the features "radius", "perimeter" and "area":
```{r pca shape}
mesures.shape <- c("radius", "perimeter", "area")
measures <- c("texture","smoothness", "compactness", "concavity", "concave_pts", "symmetry", "fractal_dim")
shape <- c()
for (prefix in mesures.shape) {
  # Find column names that start with the current prefix
  shape <- append(shape,paste0(prefix, "_mean"))
  shape <- append(shape,paste0(prefix, "_se"))
  shape <- append(shape,paste0(prefix, "_worst"))
}
brca.shape <- brca.x.normal[,shape]
pca <- prcomp(brca.shape)
summary(pca)
```
By taking PC1 and PC2 alone (instead all the 6 features) we can explain nearly 98% of the variance.
therefore, we will create 2 new features: "shape_pc1" and "shape_pc2" instead the previous ones.
```{r}
brca.reduce <- data.frame(brca[,c('diagnosis')])
colnames(brca.reduce) <- c("diagnosis")
brca.reduce$shape_pc1 <- pca$x[,1]
brca.reduce$shape_pc2 <- pca$x[,2]
```
2) preform PCA on every triplet of "mean", "standard error" and "worst".
we replace the original features with PC1 and PC2 if they can explain more than 95% of the variance.
```{r}
for (prefix in measures) {
  # Find column names that start with the current prefix
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")
  brca.subset <- brca.x.normal[,c(prefix.mean, prefix.se, prefix.worst)]
  cat("--------", prefix, "--------\n")
  pca <- prcomp(brca.subset)
  exp_pc2_var <- sum(pca$sdev[1:2]^2/sum(pca$sdev^2))
  if (exp_pc2_var < 0.95) {
    cat("PC2 Cumulative Proportion of Variance:", exp_pc2_var, "\nNot Preforming dimentionality reduction\n")
    brca.reduce[[paste0(prefix, "_mean")]] <- brca.x.normal[,prefix.mean]
    brca.reduce[[paste0(prefix, "_se")]] <- brca.x.normal[,prefix.se]
    brca.reduce[[paste0(prefix, "_worst")]] <- brca.x.normal[,prefix.worst]
  }
  else{
    cat(prefix, ": PC2 Cumulative Proportion of Variance:", exp_pc2_var, "\nreplace", prefix,"mean, se and worst to PC1 and PC2\n")
    brca.reduce[[paste0(prefix, "_pc1")]] <- pca$x[,1]
    brca.reduce[[paste0(prefix, "_pc2")]] <- pca$x[,2]
  }
}
brca.x.reduce <- brca.reduce[,!(colnames(brca.reduce) %in% c("diagnosis"))]
```
Now we check the correlation matrix between the features in the reduced data
```{r corr after pca}
corrs <- cor(brca.x.reduce)
colnames(corrs) <- gsub("_", " ", colnames(corrs))
rownames(corrs) <- gsub("_", " ", rownames(corrs))
corrplot(corrs, method = "square", tl.col = "black", tl.srt = 45, sig.level = 0.05, type = 'lower', diag = FALSE)
```
During this analysis we will apply our methods over the raw data and the reduced version and examine if the reduced version will deliver better results.
To visually see the two data sets we plot t-SNE on both sets, and color the observations according to the disgnosis:
```{r}
#install.packages("gridExtra")
library(gridExtra)
library(Rtsne)
reduce.tsne <- Rtsne(brca.x.reduce,
  pca = FALSE, perplexity = 10,
  theta = 0.0
)
reduce.tsne <- data.frame(
  TSNE1 = reduce.tsne$Y[, 1],
  TSNE2 = reduce.tsne$Y[, 2],
  Diagnosis = brca$diagnosis)
p1 <- ggplot(reduce.tsne, aes(
  x = TSNE1, y = TSNE2,
  col = Diagnosis
)) +
  geom_point() + ggtitle("After dimantionality reduction") + theme(legend.position = "none")
full.tsne <- Rtsne(brca.x,
  pca = FALSE, perplexity = 10,
  theta = 0.0
)
full.tsne <- data.frame(
  TSNE1 = full.tsne$Y[, 1],
  TSNE2 = full.tsne$Y[, 2],
  Diagnosis = brca$diagnosis)
p2 <- ggplot(full.tsne, aes(
  x = TSNE1, y = TSNE2,
  col = Diagnosis
)) +
  geom_point()  + ggtitle("Before dimantionality reduction")
grid.arrange(p2, p1, widths=c(0.55, 0.45), ncol=2)
```


### supplementary Plots:
correlation between triplets of "mean", "se" and "worst".
```{r corr triplets plot}
for (prefix in measures) {
  # Find column names that start with the current prefix
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")
  brca.subset <- brca.x.normal[,c(prefix.mean, prefix.se, prefix.worst)]
  colnames(brca.subset) <- gsub("_", " ", colnames(brca.subset))
  cor(brca.subset) %>% corrplot(method = "square", tl.col = "black", tl.srt = 45, sig.level = 0.05, type = 'lower', diag = FALSE)
}
```
correlations between all the features that related to the shape of the tumor cells (radius, perimeter and area)
```{r corr shape plot}
colnames(brca.shape) <- gsub("_", " ", colnames(brca.shape))
cor(brca.shape) %>%
corrplot(method = "square", tl.col = "black", tl.srt = 45,
         sig.level = 0.05, type = 'lower', diag = FALSE)
```

** Applying Different Machine Learning Algorithms On The Data: **

# KNN Algorithm

First we will check the knn algorithm on the raw data(before performing PCA:

```{r}
library(caret)
# Shuffle the data:
shuffled_data <- brca.normal[sample(nrow(brca.normal)), ]

# Create index to split based on labels  
index <- createDataPartition(shuffled_data$diagnosis, p=0.8, list=FALSE)

# Subset training set with index
brca.normal.training <- shuffled_data[index,]

# Subset test set with index
brca.normal.test <- shuffled_data[-index,]


# Train a model
model_knn <- train(brca.normal.training[, 2:31], brca.normal.training[, 1], method='knn')


# Access the optimal k value
best_k <- model_knn$bestTune$k
# Print the best k value
cat("Best k is:", best_k, "\n")

# Predict the labels of the test set
predictions<-predict.train(object=model_knn,brca.normal.test[,2:31], type="raw")

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,brca.normal.test[,1])

```

Now we will check the knn algorithm on the reduced data:
```{r}
# Shuffle the data:
shuffled_data <- brca.reduce[sample(nrow(brca.reduce)), ]

# Create index to split based on labels  
index <- createDataPartition(shuffled_data$diagnosis, p=0.8, list=FALSE)

# Subset training set with index
brca.reduced.training <- shuffled_data[index,]

# Subset test set with index
brca.reduced.test <- shuffled_data[-index,]



# Train a model
model_knn <- train(brca.reduce.training[, 2:20], brca.reduce.training[, 1], method='knn')


# Access the optimal k value
best_k <- model_knn$bestTune$k
# Print the best k value
cat("Best k is:", best_k, "\n")

# Predict the labels of the test set
predictions<-predict.train(object=model_knn,brca.reduce.test[,2:20], type="raw")

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,brca.reduce.test[,1])

```
As we can see, the accuracy of the knn model improved after dimensionality reduction.

# SVM algorithm

We used the default hyper parameters, since they gave us the best results. We split the data into train set
(450 samples, 80%) and test set (119 samples, 20%)
Trying SVM on the raw data:
```{r}
library(e1071)
library(gmodels)
library(caret)


# Subset training set with index
train_sample <- sample(569, 456)
brca.normal.training<- brca.x.normal[train_sample, ]
brca.normal.training.labels <- as.factor(brca.normal$diagnosis[train_sample])
brca.normal.test <- brca.x.normal[-train_sample, ]
brca.normal.test.labels <- as.factor(brca.normal$diagnosis[-train_sample])


# Train the SVM model
regressor_svm <- svm(formula = brca.normal.training.labels ~ .,
                     data = brca.normal.training,
                     type = 'C-classification',
                     kernel = 'linear')

# Make predictions on the test set
y_pred1 <- predict(regressor_svm, newdata = brca.normal.test)



# Calculate and print SVM accuracy
accuracy <- sum(y_pred1 == brca.normal.test.labels) / length(brca.normal.test.labels)
print(paste('accuracy:', accuracy))

# Create a contingency table
CrossTable(brca.normal.test.labels, y_pred1, prop.chisq = FALSE, prop.c = FALSE, 
           prop.r = FALSE, dnn = c('actual type', 'predicted type'))

```

```{r}
# Extract SVM coefficients and SVs
SVM_coefs <- t(regressor_svm$coefs) %*% regressor_svm$SV

# Convert SVM coefficients to a data frame for easier plotting
coefs_df <- data.frame(factors = colnames(SVM_coefs), importance = t(SVM_coefs))

# Sort the factors by their importance
coefs_df <- coefs_df[order(coefs_df$importance, decreasing = TRUE), ]

# Create a bar plot of factors' importance with colors and rotated labels
barplot(coefs_df$importance, names.arg = coefs_df$factors, 
        main = "SVM Factors Importance", ylab = "Importance",
        col =  "purple", las = 2, cex.names = 0.55)

```

trying svm on the reduced data:
```{r}


# Subset training set with index
train_sample <- sample(569, 456)
brca.x.reduce.training<- brca.x.reduce[train_sample, ]
brca.x.reduce.training.labels <- as.factor(brca.reduce$diagnosis[train_sample])
brca.x.reduce.test <-brca.x.reduce[-train_sample, ]
brca.x.reduce.test.labels <- as.factor(brca.reduce$diagnosis[-train_sample])


# Train the SVM model
regressor_svm <- svm(formula = brca.x.reduce.training.labels ~ .,
                     data = brca.x.reduce.training,
                     type = 'C-classification',
                     kernel = 'linear')

# Make predictions on the test set
y_pred1 <- predict(regressor_svm, newdata = brca.x.reduce.test)



# Calculate and print SVM accuracy
accuracy <- sum(y_pred1 == brca.x.reduce.test.labels) / length(brca.x.reduce.test.labels)
print(paste('accuracy:', accuracy))

# Create a contingency table
CrossTable(brca.x.reduce.test.labels, y_pred1, prop.chisq = FALSE, prop.c = FALSE, 
           prop.r = FALSE, dnn = c('actual type', 'predicted type'))


```

```{r}
# Extract SVM coefficients and SVs
SVM_coefs <- t(regressor_svm$coefs) %*% regressor_svm$SV

# Convert SVM coefficients to a data frame for easier plotting
coefs_df <- data.frame(factors = colnames(SVM_coefs), importance = t(SVM_coefs))

# Sort the factors by their importance
coefs_df <- coefs_df[order(coefs_df$importance, decreasing = TRUE), ]
# Define colors for the bar plot

# Create a bar plot of factors' importance with colors and rotated labels
barplot(coefs_df$importance, names.arg = coefs_df$factors, 
        main = "SVM Factors Importance", ylab = "Importance",
        col = "purple", las = 2, cex.names = 0.55)


```

#Random Forest Algorithm

First we apply the algorithm on the data set without the feature reduction. 


library(randomForest)
```
```{r}
set.seed(123)

# Take 80 percent for training and shuffle the data.
train_indices <- sample(nrow(brca.normal), nrow(brca.normal) * 0.8)
train_data <- brca.normal[train_indices, ]

# Assign the remaining data to the test set.
test_data <- brca.normal[-train_indices, ]

# Exclude the first column for training features.
train_features_normal <- train_data[, -1]

# Keep just the first column as the training target.
train_target_normal <- train_data[, 1]

# Exclude the first column for test features.
test_features_normal <- test_data[, -1]

# Keep just the first column as the test target.
test_target_normal <- test_data[, 1]

```

```{r}
set.seed(123)

# Train the random forest models with different parameters
num_trees <- c(50, 100, 150)  # Different numbers of trees
max_depth <- c(5, 10, 15)  # Different depths
results <- list()

for (n in num_trees) {
  for (d in max_depth) {
    model <- randomForest(train_features_normal, train_target_normal, ntree = n, maxdepth = d)
    predictions <- predict(model, test_features_normal)
    accuracy_normal <- sum(predictions == test_target_normal) / length(test_target_normal)
    
    result <- list(ntrees = n, maxdepth = d, model = model, predictions = predictions, accuracy = accuracy_normal)
    results <- c(results, list(result))
  }
}

# Find the model with the highest accuracy
best_result <- results[[which.max(sapply(results, function(x) x$accuracy))]]
best_model_normal <- best_result$model
best_predictions <- best_result$predictions

```
```{r}
# Get feature importance data using gini importance
importance <- importance(best_model_normal, type=2)
importance_df <- data.frame(Feature = row.names(importance), Importance = importance[, 1])
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot feature importance
importance_plot <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

plot(importance_plot)

```

After that we apply the algorithm on the data set after the feature reduction. 
```{r}
set.seed(123)

# Take 80 percent for training and shuffle the data.
train_indices <- sample(nrow(brca.reduce), nrow(brca.reduce) * 0.8)
train_data <- brca.reduce[train_indices, ]

# Assign the remaining data to the test set.
test_data <- brca.reduce[-train_indices, ]

# Exclude the first column for training features.
train_features_reduce <- train_data[, -1]

# Keep just the first column as the training target.
train_target_reduce <- train_data[, 1]

# Exclude the first column for test features.
test_features_reduce <- test_data[, -1]

# Keep just the first column as the test target.
test_target_reduce <- test_data[, 1]


```
```{r}
set.seed(123)

# Train the random forest models with different parameters for brca.reduce
num_trees_reduce <- c(50, 100, 150)  # Different numbers of trees
max_depth_reduce <- c(5, 10, 15)  # Different depths
results_reduce <- list()

for (n in num_trees_reduce) {
  for (d in max_depth_reduce) {
    model_reduce <- randomForest(train_features_reduce, train_target_reduce, ntree = n, maxdepth = d)
    predictions_reduce <- predict(model_reduce, test_features_reduce)
    accuracy_reduce <- sum(predictions_reduce == test_target_reduce) / length(test_target_reduce)
    
    result_reduce <- list(ntrees = n, maxdepth = d, model = model_reduce, predictions = predictions_reduce, accuracy = accuracy_reduce)
    results_reduce <- c(results_reduce, list(result_reduce))
  }
}

# Find the model with the highest accuracy for brca.reduce
best_result_reduce <- results_reduce[[which.max(sapply(results_reduce, function(x) x$accuracy))]]
best_model_reduce <- best_result_reduce$model
best_predictions_reduce <- best_result_reduce$predictions


```
```{r}
# Get feature importance data using gini importance for brca.reduce
importance_reduce <- importance(best_model_reduce, type=2)
importance_df_reduce <- data.frame(Feature = row.names(importance_reduce), Importance = importance_reduce[, 1])
importance_df_reduce <- importance_df_reduce[order(importance_df_reduce$Importance, decreasing = TRUE), ]

# Plot feature importance for brca.reduce
importance_plot_reduce <- ggplot(importance_df_reduce, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

plot(importance_plot_reduce)
```
Now we comapre the accuracy of the 2 models that were build on two different data sets. 
```{r}
# Compare the accuracies
performance <- data.frame(Dataset = c("brca.normal", "brca.reduce"),
                          Accuracy = c(accuracy_normal, accuracy_reduce))

performance

```
We can see that the accuracy of the model that were trained on the data set after feature selection and PCA were higher. 

#XGBOOST
```{r}
library(xgboost)
```

We now apply xg boost algorithm on the two data sets.
```{r}
# Convert labels to numeric values
train_target_numeric <- ifelse(train_target_normal == "Benign", 0, 1)
test_target_numeric <- ifelse(test_target_normal == "Benign", 0, 1)

# Train the xgboost model
xgb_model_normal <- xgboost(data = as.matrix(train_features_normal), label = train_target_numeric, nrounds = 200, objective = "binary:logistic")

# Make predictions on the test set
predictions <- predict(xgb_model_normal, as.matrix(test_features_normal))

# Convert predictions to class labels
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)

# Calculate accuracy
accuracy_normal <- sum(predicted_labels == test_target_numeric) / length(test_target_numeric)

# Calculate precision
precision <- sum(predicted_labels == 1 & test_target_numeric == 1) / sum(predicted_labels == 1)

# Calculate recall
recall <- sum(predicted_labels == 1 & test_target_numeric == 1) / sum(test_target_numeric == 1)

# Calculate F1-score
f1_score <- 2 * precision * recall / (precision + recall)

# Calculate AUC-ROC
roc <- roc(test_target_numeric, predictions)
auc_roc <- auc(roc)

# Print performance metrics
cat("Performance Metrics for brca.normal dataset:\n")
cat("Accuracy:", accuracy_normal, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
cat("AUC-ROC:", auc_roc, "\n")

```
we can see that logloss started to converge after 158 rounds. 
```{r}
library(pROC)
```
Feature importance for the normal data model 
```{r}
# Get feature importance matrix
importance_matrix <- xgb.importance(model = xgb_model_normal)

# Convert importance matrix to data.frame
importance_df <- data.frame(Feature = importance_matrix$Feature, Importance = importance_matrix$Gain)

# Sort the importance in descending order
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot feature importance
importance_plot <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

print(importance_plot)

```

```{r}

set.seed(123)
# Convert labels to numeric values
train_target_numeric <- ifelse(train_target_reduce == "Benign", 0, 1)
test_target_numeric <- ifelse(test_target_reduce == "Benign", 0, 1)

# Train the XGBoost model on brca.reduce
xgb_model_reduce <- xgboost(data = as.matrix(train_features_reduce), label = train_target_numeric, nrounds = 300, objective = "binary:logistic")

# Make predictions on test set
predictions_reduce <- predict(xgb_model_reduce, as.matrix(test_features_reduce))

# Calculate accuracy
accuracy_reduce <- sum(round(predictions_reduce) == test_target_numeric) / length(test_target_numeric)

# Calculate precision
precision <- sum(predicted_labels == 1 & test_target_numeric == 1) / sum(predicted_labels == 1)

# Calculate recall
recall <- sum(predicted_labels == 1 & test_target_numeric == 1) / sum(test_target_numeric == 1)

# Calculate F1-score
f1_score <- 2 * precision * recall / (precision + recall)

# Calculate AUC-ROC
roc <- roc(test_target_numeric, predictions)
auc_roc <- auc(roc)

# Print performance metrics
cat("Performance Metrics for brca.normal dataset:\n")
cat("Accuracy:", accuracy_reduce, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
cat("AUC-ROC:", auc_roc, "\n")

```
Feature importance for the reudced features model 
```{r}
# Get feature importance matrix
importance_matrix <- xgb.importance(model = xgb_model_reduce)

# Convert importance matrix to data.frame
importance_df <- data.frame(Feature = importance_matrix$Feature, Importance = importance_matrix$Gain)

# Sort the importance in descending order
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot feature importance
importance_plot <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

print(importance_plot)
```
We can see that in both of the model the most significat feature for classification is the area (shape?) and the concave. 
```{r}
# Compare the accuracies
performance_xgb <- data.frame(Dataset = c("brca.normal", "brca.reduce"),
                              Accuracy = c(accuracy_normal, accuracy_reduce))

performance_xgb

```
Also here we can see that the performance of the reduces data set is better.
