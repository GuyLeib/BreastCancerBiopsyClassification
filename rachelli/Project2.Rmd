---
title: "Final Project"
author: "Tamar Saad 207256991 & Or Arbel 207016098 & Rachel Weinberger 208812628"
date: "31/07/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this report we explored a breast cancer dataset. This dataset contains data that was extracted from pictures of nuclei in breast mass, and 10 different features were taken: size, texture, etc. Since every picture contains many nuclei, each feature has 3 values- mean, worst and standard error, making total of 30 features. In this report we will go through data processing, feature selection and machine learning tools.
Due to limitation of the number of pages, we only include here our final decisions of the options we used- normalization method, ML algorithms choices etc. 

```{r workspace, results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(plyr)
library(dplyr)
library(stringr)
library(gplots)
library(ggplot2)
library(forcats)
library(data.table)
library(reshape2)
library(affycoretools)
library(factoextra)
library(ggvis)
library(class)
library(gmodels)
library(plotly)
library(C50)
library(tidyverse)
library(skimr)     
library(ggthemes)
library(patchwork)  
library(GGally)     
library(corrplot)
library(psych)     
library(factoextra) 
library(regclass)
library(fmsb)
library(rpart.plot)
library(e1071)
library(plot.matrix)
library(gmodels)
library(plotly)
library(tidymodels)
library(rsample) 
library(ranger)
library(caret)
library(kknn)

```

Upload and examine the data
```{r, message=FALSE, warning=FALSE}
data<-read.csv("data.csv")
data<-subset(data, select=-X)
str(data)
```
In this dataset we have 30 different features of 569 samples of breast cancer. We actually have 32 columns, but one of them is the ID of the sample and one is the diagnosis, which is the classification and therefore are not considered features.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
clean_data<-data[,-1]
clean_data$diagnosis<-as.factor(clean_data$diagnosis)
clean_data <- clean_data %>%
 mutate(diagnosis = if_else(diagnosis == "B", "Benign", "Malignant")) %>%
  mutate_at(vars(diagnosis), as.factor)
clean_data <- clean_data %>% relocate(diagnosis,.after= fractal_dimension_worst)
#check how many nans we have
print(paste("number of NAs in dataset:",sum(is.na(clean_data))))
```
We don't have NAs at all, so there are no missing values.

Lets look at the classification distribution:

```{r, message=FALSE, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center', }
palette_ro = c("#ee2f35", "#fa7211", "#fbd600", "#75c731", "#1fb86e", "#0488cf", "#7b44ab")

p1 <- ggplot(clean_data, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(stat = "count", position = "stack", show.legend = FALSE) +
  scale_fill_manual(values = c(palette_ro[2], palette_ro[7])) +
  theme_minimal(base_size = 16) +
  geom_label(stat = "count", aes(label = ..count..), position = position_stack(vjust = 0.5),
             size = 5, show.legend = FALSE)

p1 +
  plot_annotation(title = "Distribution of the objective variable (diagnosis)")
```
The distribution of the objective variable is not 1:1 but biased. We decided to continue normally, and apply changes only if we'll face problems in the future.

Checking the distribution of the features:

We divided the features into 3 different groups:

mean values, standard error values and worst values.

We wanted to check if the distribution's differences between malignant and benign diagnosis.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
df_t_m <- as_tibble(scale(select(clean_data, -diagnosis))) %>%
  select(ends_with("_mean")) %>%
  rename_all(~ str_replace_all(., "_mean", "")) %>%
  rename_all(~ str_replace_all(., "_", "\n")) %>%
  rename_all(~ str_replace_all(., " ", "\n")) %>%
  mutate(clean_data["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")

df_t_s <- as_tibble(scale(select(clean_data, -diagnosis))) %>%
  select(ends_with("_se")) %>%
  rename_all(~ str_replace_all(., "_se", "")) %>%
  rename_all(~ str_replace_all(., "_", "\n")) %>%
  rename_all(~ str_replace_all(., " ", "\n")) %>%
  mutate(clean_data["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")

df_t_w <- as_tibble(scale(select(clean_data, -diagnosis))) %>%
  select(ends_with("_worst")) %>%
  rename_all(~ str_replace_all(., "_worst", "")) %>%
  rename_all(~ str_replace_all(., "_", "\n")) %>%
  rename_all(~ str_replace_all(., " ", "\n")) %>%
  mutate(clean_data["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")

#Checking the distribution:
GeomSplitViolin <- ggproto("GeomSplitViolin", GeomViolin, draw_group = function(self, data,
                                                              ..., draw_quantiles = NULL) {
    # Original function by Jan Gleixner (@jan-glx)
    # Adjustments by Wouter van der Bijl (@Axeman)
    data <- transform(data, xminv = x - violinwidth * (x - xmin), xmaxv = x + violinwidth
                      * (xmax - x))
    grp <- data[1, "group"]
    newdata <- plyr::arrange(transform(data, x = if
                                  (grp %% 2 == 1) xminv else xmaxv), if (grp %% 2 == 1) y else -y)
    newdata <- rbind(newdata[1, ], newdata, newdata[nrow(newdata), ], newdata[1, ])
    newdata[c(1, nrow(newdata) - 1, nrow(newdata)), "x"] <- round(newdata[1, "x"])
    if (length(draw_quantiles) > 0 & !scales::zero_range(range(data$y))) {
      stopifnot(all(draw_quantiles >= 0), all(draw_quantiles <= 1))
      quantiles <- create_quantile_segment_frame(data, draw_quantiles, split = TRUE, grp = grp)
      aesthetics <- data[rep(1, nrow(quantiles)), setdiff(names(data), c("x", "y")), drop = FALSE]
      aesthetics$alpha <- rep(1, nrow(quantiles))
      both <- cbind(quantiles, aesthetics)
      quantile_grob <- GeomPath$draw_panel(both, ...)
      ggplot2:::ggname("geom_split_violin", grid::grobTree(GeomPolygon$draw_panel(newdata, ...),
                                                           quantile_grob))
    }
    else {
      ggplot2:::ggname("geom_split_violin", GeomPolygon$draw_panel(newdata, ...))
    }
  }
)
create_quantile_segment_frame <- function(data, draw_quantiles, split = FALSE, grp = NULL) {
  dens <- cumsum(data$density) / sum(data$density)
  ecdf <- stats::approxfun(dens, data$y)
  ys <- ecdf(draw_quantiles)
  violin.xminvs <- (stats::approxfun(data$y, data$xminv))(ys)
  violin.xmaxvs <- (stats::approxfun(data$y, data$xmaxv))(ys)
  violin.xs <- (stats::approxfun(data$y, data$x))(ys)
  if (grp %% 2 == 0) {
    data.frame(
      x = ggplot2:::interleave(violin.xs, violin.xmaxvs),
      y = rep(ys, each = 2), group = rep(ys, each = 2)
    )
  } else {
        data.frame(
      x = ggplot2:::interleave(violin.xminvs, violin.xs),
      y = rep(ys, each = 2), group = rep(ys, each = 2)
    )
  }
}
geom_split_violin <- function(mapping = NULL, data = NULL, stat = "ydensity", position = "identity",
                              ..., 
                              draw_quantiles = NULL, trim = TRUE, scale = "area", na.rm = FALSE, 
                              show.legend = NA, inherit.aes = TRUE) {
  layer(data = data, mapping = mapping, stat = stat, geom = GeomSplitViolin, position = position, 
        show.legend = show.legend, inherit.aes = inherit.aes, 
        params = list(trim = trim, scale = scale, draw_quantiles = draw_quantiles, na.rm = na.rm, ...))
}
```

Visualize the results for mean values, with jitter plot and box plot

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.show='hold', out.width="50%", out.height="50%"}

#Jitter plot
p1 <- ggplot(df_t_m,
       aes(factor(features,
                  levels = c("radius", "texture", "perimeter", "area", "smoothness",
                             "compactness", "concavity", "concave\npoints", "symmetry",
                             "fractal\ndimension")),
           value, fill = diagnosis)) +
  geom_jitter(aes(colour = diagnosis)) +
  scale_colour_manual(values = c(palette_ro[2], palette_ro[7])) +
  labs(x = "explanatory variables", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))


#box plot
p2 <- ggplot(df_t_m,
       aes(factor(features,
                  levels = c("radius", "texture", "perimeter", "area", "smoothness",
                             "compactness", "concavity", "concave\npoints", "symmetry",
                             "fractal\ndimension")),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c(palette_ro[2], palette_ro[7])) +
  labs(x = "explanatory variables", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))

p1 +
  plot_annotation(title = "Mean values distribution")
p2 +
  plot_annotation(title = "Mean values distribution")

```

Visualize the results for standard error values

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.show='hold', out.width="50%", out.height="50%", echo=FALSE}
#jitter plot
p1 <- ggplot(df_t_s,
       aes(factor(features,
                  levels = c("radius", "texture", "perimeter", "area", "smoothness",
                             "compactness", "concavity", "concave\npoints", "symmetry",
                             "fractal\ndimension")),
           value, fill = diagnosis)) +
  geom_jitter(aes(colour = diagnosis)) +
  scale_colour_manual(values = c(palette_ro[2], palette_ro[7])) +
  labs(x = "explanatory variables", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))

p1 +
  plot_annotation(title = "Standard error values distribution")

#box plot
p1 <- ggplot(df_t_s,
       aes(factor(features,
                  levels = c("radius", "texture", "perimeter", "area", "smoothness",
                             "compactness", "concavity", "concave\npoints", "symmetry", "fractal\ndimension")),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c(palette_ro[2], palette_ro[7])) +
  labs(x = "explanatory variables", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))

p1 +
  plot_annotation(title = "Standard error values distribution")
```

Visualize the results for worst values

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.show='hold', out.width="50%", out.height="50%", echo=FALSE}
#jitter plot
p1 <- ggplot(df_t_w,
       aes(factor(features,
                  levels = c("radius", "texture", "perimeter", "area", "smoothness",
                             "compactness", "concavity", "concave\npoints", "symmetry", "fractal\ndimension")),
           value, fill = diagnosis)) +
  geom_jitter(aes(colour = diagnosis)) +
  scale_colour_manual(values = c(palette_ro[2], palette_ro[7])) +
  labs(x = "explanatory variables", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))

p1 +
  plot_annotation(title = "Worst values distribution")

#box plot
p1 <- ggplot(df_t_w,
       aes(factor(features,
                  levels = c("radius", "texture", "perimeter", "area", "smoothness",
                             "compactness", "concavity", "concave\npoints", "symmetry",
                             "fractal\ndimension")),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c(palette_ro[2], palette_ro[7])) +
  labs(x = "explanatory variables", y = "standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7))

p1 +
  plot_annotation(title = "Worst values distribution")
```
We can see for all the groups that not all features has the same distribution of malignant and benign samples. It means that there are differences between the features of the groups, so maybe ML tools could help us to classify the samples.

Next, we wanted to see if there are correlations between the features.

Correlation matrix:

```{r, message=FALSE, warning=FALSE, out.width="70%", out.height="70%", fig.align='center'}
cor(clean_data[,-(ncol(clean_data))]) %>%
  corrplot(method = "square", tl.col = "black", tl.srt = 45,
           sig.level = 0.05)

```
We can see that there are correlations mostly in the areas of features from the same type:

Worst values with worst values, and mean values with mean values.
The big dark squares make us suspect that perhaps we have multicollinearity, which can deflect the ML algorithms.

In order to check if we indeed have multicolliearirty, we used VIF.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
X<-clean_data[,-(ncol(clean_data))]
print(paste("the VIF value of radius mean is:",VIF(lm(radius_mean ~ .,data=X))))
print(paste("the VIF value of texture mean is:",VIF(lm(texture_mean ~ .,data=X))))

require(fmsb)
vif_func <- function(in_frame,thresh=10, trace=F,...){
  require(fmsb)
  if(class(in_frame) != 'data.frame') in_frame<-data.frame(in_frame)
  vif_init <- vector('list', length = ncol(in_frame))
  names(vif_init) <- names(in_frame)
  var_names <- names(in_frame)
  
  for(val in var_names){
    regressors <- var_names[-which(var_names == val)]
    form <- paste(regressors, collapse = '+')
    form_in <- formula(paste(val,' ~ .'))
    vif_init[[val]] <- VIF(lm(form_in,data=in_frame,...))
  }
  vif_max<-max(unlist(vif_init))
   if(vif_max < thresh){

    if(trace==T){ #print output of each iteration
      prmatrix(vif_init,collab=c('var','vif'),rowlab=rep('', times = nrow(vif_init) ),quote=F)
      cat('\n')
      cat(paste('All variables have VIF < ', thresh,', max VIF ',round(vif_max,2), sep=''),'\n\n')
    }
    return(names(in_frame))
  }
  else{
    in_dat<-in_frame
    #backwards selection of explanatory variables, stops when all VIF values are below 'thresh'
    while(vif_max >= thresh){

      vif_vals <- vector('list', length = ncol(in_dat))
      names(vif_vals) <- names(in_dat)
      var_names <- names(in_dat)
      
      for(val in var_names){
        regressors <- var_names[-which(var_names == val)]
        form <- paste(regressors, collapse = '+')
        form_in <- formula(paste(val,' ~ .'))
        vif_add <- VIF(lm(form_in,data=in_dat,...))
        vif_vals[[val]] <- vif_add
      }
      max_row <- which.max(vif_vals)

      vif_max<-vif_vals[max_row]
      
      if(vif_max<thresh) break

      if(trace==T){ #print output of each iteration
        vif_vals <- do.call('rbind', vif_vals)
        vif_vals
        prmatrix(vif_vals,collab='vif',rowlab=row.names(vif_vals),quote=F)
        cat('\n')
        cat('removed: ', names(vif_max),unlist(vif_max),'\n\n')
        flush.console()
      }
       in_dat<-in_dat[,!names(in_dat) %in% names(vif_max)]
    }
    return(names(in_dat))
  }
}

```
As we can see, the VIF scores are very high:

If the largest VIF value is 5 or more, multicollinearity exists. If it is 10 or more, multicollinearity is evaluated as serious. Therefore, we wanted to remove the correlated values.

We set the VIF threshold to 10, since many of the features were removed with this threshold already, and we didn't want to lose too much data. We decided to go with this threshold, and if we faced problems with classification in the future- change it respectively.
After selecting the remaining features we created the correlation matrix again to check the results.

```{r, message=FALSE, warning=FALSE, results='hide'}
# Clearing columns with high multicollinearity
data_custom <- vif_func(X, thresh=10, trace=T)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center'}
print(paste("Number of remaining features:",length(data_custom)))

X_2 <- X[, data_custom]

cor(X_2) %>%
  corrplot(method = "square", tl.col = "black", tl.srt = 45,
           sig.level = 0.05)

```
As we can see, there is a significantly smaller number of correlated features, as wanted.

# **Normalization:**

We normalized by min-max and by Z-score, and wanted to check which one gave better results. 

### Normalize the data by MIN/MAX:

```{r}
# create the min/max normalization function:
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#normalize the data without the classification column
minmax_data <- as.data.frame(lapply(X_2, normalize))
str(minmax_data)
```
### Normalize the data by z-score:


```{r, message=FALSE, warning=FALSE}
# z-score normalization
zscore_data <- as.data.frame(scale(X_2))
str(zscore_data)
```

We tried to see which of the normalization gives us the best clustering results.

We performed dimension reduction with PCA, and applied loading arrows to see the weight of every feature an the principle components.

With Min-Max normalization:

```{r, message=FALSE, warning=FALSE, fig.show='hold', out.width="50%", out.height="50%"}

pca <- prcomp(minmax_data)


pca_to_show <- data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  classification = as.factor(clean_data$diagnosis)
)

ggplot(pca_to_show, aes(x = PC1, y = PC2, col = classification)) +
  geom_point()

fviz_pca_var(pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )


```
With Z-Score normalization:

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.show='hold', out.width="50%", out.height="50%"}

pca <- prcomp(zscore_data)
pca_to_show <- data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  classification = as.factor(clean_data$diagnosis)
)

ggplot(pca_to_show, aes(x = PC1, y = PC2, col = classification)) +
  geom_point()
fviz_pca_var(pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
There is a good classification in both normalization methods, so we can assume that ML tools will be effective in analyzing the data.

When applying both z-score and minmax, we got similar results, but z-score gave us slightly better results. Therefore, we decided to continue with Z-score normalization.


#  **Machine learning algorithms:**

We chose to use the following algorithms:

* SVM

* Decision Tree

* Random Forest

* KNN

We chose to use 2 linear algorithms (SVM & KNN) because we saw good clustering in PCA, which is a linear algorithm for dimension reduction. We therefore concluded that linear models could help us in classification.

In addition, we wanted to apply non-linear algorithms as well, which is why we also included DT and RF.


# SVM algorithm:

We used the default hyper parameters, since they gave us the best results.
We split the data into train set (450 samples, 80%) and test set (119 samples, 20%)
```{r, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center'}
# split to train and test 
set.seed(1)
train_sample <- sample(569, 450)
zscore_train <- zscore_data[train_sample, ]
zscore_train_labels <- as.factor(data$diagnosis[train_sample])
zscore_test <- zscore_data[-train_sample, ]
zscore_test_labels <- data$diagnosis[-train_sample]
regressor_svm <- svm(formula = zscore_train_labels ~ ., 
                    data=zscore_train,
                    type = 'C-classification',
                    kernel = 'linear')
y_pred1 = predict(regressor_svm, newdata = zscore_test)
# Svm's accuracy
print((paste('accuracy:',sum(y_pred1 == zscore_test_labels)/length(zscore_test_labels)*100)))
CrossTable(zscore_test_labels, y_pred1, prop.chisq = FALSE, prop.c = FALSE, 
           prop.r = FALSE, dnn = c('actual type', 'predicted type'))
plot_dataframe <- zscore_train
plot_dataframe$diagnosis <- zscore_train_labels
par(mar=c(15, 0, 5, 0))
SVM_factors_importance = t(regressor_svm$coefs) %*% regressor_svm$SV
plot(SVM_factors_importance, ylab="", yaxt="n", 
     xlab="", axis.col=list(side=1, las=2), fmt.cell='%.3f', cex=0.7, cex.axis= 0.7, 
     width=400, height=20, main=NULL, col=topo.colors)
```

As we can see, SVM gave us pretty good result on the test set.

The most important factors were area_worst and concave.points_mean.


# Decision Tree (rpart) algorithm:

To prevent over fitting, we used the hyper parameters minsplit = 50, maxdepth = 3, and cp = 0.001.

We split the data into train set (450 samples, 80%) and test set (119 samples, 20%).

The outcomes are pretty good- TP=0.954.

Each node shows:

- the predicted class,

- the predicted probability to be Malignant,

- the percentage of observations in the node.

```{r, warning=FALSE, out.width="70%", out.height="70%", fig.align='center'}
set.seed(0)
diagnosis = data$diagnosis
cart <- rpart(diagnosis ~ .,
              data = zscore_data, method = "class",
              control=rpart.control(minsplit=50,
                                    maxdepth=3,
                                    cp=0.001))
p <- predict(cart, type="class")
```

```{r, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center'}
print(paste0('accuracy: ',sum(p == diagnosis)/length(diagnosis)*100))

CrossTable(diagnosis, p, prop.chisq = FALSE, prop.c = FALSE, 
           prop.r = FALSE, dnn = c('actual type', 'predicted type'))
rpart.plot(cart)

```

# RANDOM FOREST:
We split the data into train set (427 samples, 75%) and test set (142 samples, 25%)

```{r, echo=FALSE}
#split the data to train and test
set.seed(0)
zscore_data$diagnosis<-clean_data$diagnosis
data_split_z <- initial_split(zscore_data, prop = 3/4 , strata = diagnosis )
train_z <- training(data_split_z)
test_z <- testing(data_split_z)

zscore_train<-zscore_train
zscore_test<-zscore_test
```

We run the model with 3 different options: 500 trees, 900 trees, and 1500 trees.
We chose those numbers because they are big enough to enable the models learning, but not too big to increase the running time and get to over fitting.
```{r}
set.seed(0)
# the model #
## z score ##
rf_z_500 <- ranger(diagnosis ~.,
             data = train_z, 
             mtry = 5, num.trees = 500, write.forest=TRUE, importance = "permutation")

rf_z_1500 <- ranger(diagnosis ~.,
             data = train_z, 
             mtry = 5, num.trees = 1500, write.forest=TRUE, importance = "permutation")

rf_z_900 <- ranger(diagnosis ~.,
             data = train_z, 
             mtry = 5, num.trees = 900, write.forest=TRUE, importance = "permutation")

```

Prediction of the model:
```{r, echo=FALSE}
pred_z_500 <- predict(rf_z_500, data=test_z)$predictions
print("Confusion matrix of Random Forest with 500 trees:")
confusionMatrix(pred_z_500, test_z$diagnosis, positive = "Malignant")


pred_z_1500 <- predict(rf_z_1500, data=test_z)$predictions
print("Confusion matrix of Random Forest with 1500 trees:")
confusionMatrix(pred_z_1500, test_z$diagnosis, positive = "Malignant")


pred_z_900 <- predict(rf_z_900, data=test_z)$predictions
print("Confusion matrix of Random Forest with 900 trees:")
confusionMatrix(pred_z_900, test_z$diagnosis, positive = "Malignant")
```
We can see that we received similar results from RF with 900 and 1500 trees, and also that we didn't see a difference between min-max and z-score

Feature importance of random forest:
```{r, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center'}
data.frame(variables = names(importance(rf_z_900, method = "janitza")),
           feature_importance = importance(rf_z_900, method = "janitza")) %>%
  ggplot(aes(x = feature_importance,
             y = reorder(variables, X = feature_importance))) +
    geom_bar(stat = "identity",
             fill = palette_ro[6],
             alpha=0.9) +
    labs(y = "features", title = "Feature importance of random forest") +
    theme_minimal(base_size = 12)
```
We can see here that the most important features for RF are similar to those in SVM, even though SVM is a linear model and RF is not. This repetitiveness increases the reliability of the models.

# KNN ALGORITHM:

We split the data into train set (427 samples, 75%) and test set (142 samples, 25%)

While testing the ultimate K for the algorithm, we tried different values and presented here three of them. Choosing the values, we wanted the numbers to be odd and proportionate to the size of the data.


```{r, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center'}
# check different k:
knn_z_5 <- nearest_neighbor(neighbors = 5) %>% 
  set_engine('kknn') %>% 
  set_mode('classification') %>%
  fit(diagnosis ~., data = train_z)

results_zscore_test_5 <- knn_z_5 %>% 
  predict(test_z) %>% 
  mutate(truth = test_z$diagnosis, accuracy = if_else(.pred_class == truth, "yes", "no"))

knn_z_9 <- nearest_neighbor(neighbors = 9) %>% 
  set_engine('kknn') %>% 
  set_mode('classification') %>%
  fit(diagnosis ~., data = train_z)

results_zscore_test_9 <- knn_z_9 %>% 
  predict(test_z) %>% 
  mutate(truth = test_z$diagnosis, accuracy = if_else(.pred_class == truth, "yes", "no"))

knn_z_15 <- nearest_neighbor(neighbors = 15) %>% 
  set_engine('kknn') %>% 
  set_mode('classification') %>%
  fit(diagnosis ~., data = train_z)

results_zscore_test_15 <- knn_z_15 %>% 
  predict(test_z) %>% 
  mutate(truth = test_z$diagnosis, accuracy = if_else(.pred_class == truth, "yes", "no"))

results_zscore_test <- knn_z_5 %>% 
  predict(test_z) %>% 
  mutate(k = "5", 
         truth = test_z$diagnosis) %>% 
          bind_rows(knn_z_9 %>% 
          predict(test_z) %>% 
          mutate(k = "9", truth = test_z$diagnosis)) %>% bind_rows(knn_z_15 %>% 
          predict(test_z) %>% 
          mutate(k = "15", truth = test_z$diagnosis)) %>%
  mutate(accuracy = if_else(.pred_class == truth, "yes", "no"))
results_zscore_test %>% 
  ggplot() +
  geom_bar(aes(accuracy, fill = accuracy)) +
  ggtitle("Accuracy of KNN for different K values") +
  facet_wrap(~ k) 
```

As we can see the best K is 15.

```{r, warning=FALSE, echo=FALSE,}
print("Results of KNN for K = 15:")
CrossTable(x = results_zscore_test_15$truth, y = results_zscore_test_15$.pred_class, prop.chisq=FALSE)
```

Visualize predictions on test split:

*The axes are the two most importance features as we saw in other algorithms.

```{r, warning=FALSE, echo=FALSE, out.width="70%", out.height="70%", fig.align='center'}
yscore <- knn_z_15 %>%
  predict(test_z, type = 'prob')
colnames(yscore) <- c('yscore0','yscore1')
yscore <- yscore$yscore1
pdb <- cbind(test_z, yscore)

fig <- plot_ly(data = pdb,x = ~area_worst, y = ~concave.points_mean, type = 'scatter',
               mode = 'markers',color = ~yscore, colors = 'RdBu', symbol = ~diagnosis,
               split = ~diagnosis, symbols = c('square-dot','circle-dot'), 
               marker = list(size = 12, line = list(color = 'black', width = 1)))

fig
```
# **Conclusion:**

All models gave us good results, and apparently our data is convenient to classify. 
All of the models presented similar results, with Random Forest and SVM that seemed a bit better than the rest. We would probably give up on KNN, since it's a lazy algorithm and therefore will be the slowest to use. Moreover, it gives all the features equal weights, which is an incorrect approach in many cases. 
We saw in SVM, RF, DT and PCA that the most important features were 'area worst' and 'concave points mean'. These findings can teach us a lot about our data, and should be considered while applying more algorithms or EDA. It is also interesting to examine further the effect of these features on the diagnosis.
