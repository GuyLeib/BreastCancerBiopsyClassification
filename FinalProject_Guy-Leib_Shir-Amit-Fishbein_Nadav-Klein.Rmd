---
title: "Breast Cancer Biopsy Classification Project"
subtitle: Guy Leib (316311190), Shir Amit Fishbein (207640228), Nadav Klein (318865698)
authors: Guy Leib, Shir Amit Fishbein, Nadav Klein
output:
  word_document: default
  pdf_document: default
always_allow_html: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r imports, include=FALSE}
library(ggplot2)
library(tidyverse)
library(patchwork)
library(dplyr)
library(corrplot)
library(GGally)
library(plotly)
library(gridExtra)
library(Rtsne)
library(caret)
library(e1071)
library(gmodels)
library(randomForest)
library(xgboost)
library(pROC)
library(party)
library(grid)
library(xgboost)
library(DiagrammeR)
```
# Introduction
The early detection of malignant tumors plays a critical role in improving the survival rates and quality of life for patients diagnosed with breast cancer. Breast cancer is among the most common types of cancer in women worldwide, making it a vital area for continuous research and development. This project focuses on applying Machine Learning (ML) algorithms to analyze breast mass features and accurately differentiate between benign and malignant tumors. By leveraging the computational power and predictive capabilities of ML, this approach offers a promising avenue towards more effective, personalized, and early-stage cancer diagnosis, potentially revolutionizing traditional diagnostic methods and contributing to the broader fight against this prevalent disease.

# The Data
The Data were taken from [Kaggle](https://www.kaggle.com/datasets/utkarshx27/breast-cancer-wisconsin-diagnostic-dataset):
Biopsy features for classification of 569 malignant and benign breast masses.
Features were computationally extracted from digital images of fine needle aspirate biopsy slides.

## Columns description
In each observation, 10 attributes were measured for *every cell* in the biopsy (see the list below).
Our data contain summary of those results and each row in the data contain only the mean, the standard error and the worst case among the cells (can be the larger or the most severe, depending the feature).
overall, the data contains 3*10 features and a label (column Y) that mention wherever the mass is malignant (M) or benign (B).
*list of the 10 measures:*
radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension.
radius: 	Nucleus radius (mean of distances from center to points on perimeter).
texture:	 Nucleus texture (standard deviation of grayscale values).
perimeter:	Nucleus perimeter.
area:	Nucleus area.
smoothness:	Smoothness of the tumor cells.
compactness:	 Nucleus compactness (perimeter^2/area - 1).
concavity:	Nucleus concavity (severity of concave portions of the contour).
concave_points:	Number of concave portions of the nucleus contour.
symmetry:	Nucleus Symmetry.
fractal_dimension: Nucleus fractal dimension ("coastline approximation" - 1).


# Get started
## Upload and initial cleaning of the data

<<<<<<< ADD STRUCTURE OF THE DATA>>>>>>>>
We are first changing the names of the columns and the labels from 'M' and 'B' to 'Malignant' and 'Benign'.
We first uploaded the data and changed the names of some of the columns, we also removed whitespaces and removed unnecessary columns.
The final columns names are:
```{r load, message=FALSE, warning=FALSE, echo=FALSE}
brca <-read.csv("data//brca.csv")
```

```{r cleaning, echo=FALSE}
# remove the prefix "x." from column names
colnames(brca) <- gsub("^x\\.", "", colnames(brca))
colnames(brca)[which(names(brca) == "y")] <- "diagnosis"
# remove id column
brca<-brca[,-1]
# factorize the label
brca <- brca %>% mutate(diagnosis = if_else(diagnosis == "B", "Benign", "Malignant"))
brca$diagnosis <- as.factor(brca$diagnosis)
brca.x <- brca[,!(colnames(brca) %in% c("diagnosis"))]

```

```{r}
colnames(brca)
```

## Show values range
Afterward, to examine our data, we want to see the ranges of the values within each column, and see if there are NAs:
```{r examine, echo=FALSE}
measures <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave_pts", "symmetry", "fractal_dim")
cat("Range of values in the numerical columns: \n")
for (prefix in measures) {
  # Find column names that start with the current prefix
  cat("-------", prefix, "-------\n")
  cat("\t\t\t min:\t\t max:\t\t mean:\n")
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")

  # Print the result
  cat("Mean:\t\t\t", round(min(brca[[prefix.mean]]), 2), "\t\t" , round(max(brca[[prefix.mean]]), 2), "\t\t", round(mean(brca[[prefix.mean]]) ,2), "\n")
  cat("Standard error:\t\t", round(min(brca[[prefix.se]]), 2), "\t\t" , round(max(brca[[prefix.se]]), 2), "\t\t", round(mean(brca[[prefix.se]]) ,2), "\n")
  cat("Worst:\t\t\t", round(min(brca[[prefix.worst]]), 2), "\t\t" , round(max(brca[[prefix.worst]]), 2), "\t\t", round(mean(brca[[prefix.worst]]) ,2), "\n")
}
cat("\n\nnumber of NAs in dataset: ",sum(is.na(brca)))
```
Above we see the ranges of the values in our data and that there are no missing values. all the features are numeric.
# Data cleaning
## Normalization
Because we have different ranges and we don't want that the scale of the feature will be a factor during the study we normalized each column to values between 0 and 1 by performing Min-Max normalization.
```{r norm, echo=FALSE,include=FALSE,results='hide'}
minmax <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
brca.x.normal <- as.data.frame(lapply(brca.x, minmax))
# Retrieve the diagnosis column
diagnosis <- brca$diagnosis

# Combine normalized data frame with diagnosis column
brca.normal <- cbind(diagnosis, brca.x.normal)
```
```{r show norm range, echo=FALSE, results='hide'}
for (prefix in measures) {
  # Find column names that start with the current prefix
  cat("-------", prefix, "-------\n")
  cat("\t\t\t min:\t\t max:\t\t mean:\n")
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")

  # Print the result
  cat("Mean:\t\t\t", round(min(brca.x.normal[[prefix.mean]]), 2), "\t\t" , round(max(brca.x.normal[[prefix.mean]]), 2), "\t\t", round(mean(brca.x.normal[[prefix.mean]]) ,2), "\n")
  cat("Standard error:\t\t", round(min(brca.x.normal[[prefix.se]]), 2), "\t\t" , round(max(brca.x.normal[[prefix.se]]), 2), "\t\t", round(mean(brca.x.normal[[prefix.se]]) ,2), "\n")
  cat("Worst:\t\t\t", round(min(brca.x.normal[[prefix.worst]]), 2), "\t\t" , round(max(brca.x.normal[[prefix.worst]]), 2), "\t\t", round(mean(brca.x.normal[[prefix.worst]]) ,2), "\n")
}
```

# Exploratory Data Analysis (EDA)
## Balance
First, we want to see if the labels are balanced:
```{r, echo=FALSE}
df <- brca["diagnosis"] %>% 
  group_by(diagnosis) %>%
  count() %>% 
  ungroup() %>% 
  mutate(perc = `n` / sum(`n`)) %>% 
  arrange(perc) %>%
  mutate(labels = scales::percent(perc))

ggplot(df, aes(x = "", y = perc, fill = diagnosis)) +
  geom_col() +
  geom_label(aes(label = labels),
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +
  coord_polar(theta = "y")
```
We can see that the data doesn't contain significant bias toward one of the labels.
It is possible that it could affect the results but from the other hand we don't want to lose data that can be significant.
therefore, considering that the bias is not very large, we decided to not make changes.

## distributions differences
We want to explore if we can see visually how the measures are related to the diagnosis,
we created box plots to each aspect of each measure for both labels - Benign and Malignant.
```{r mean plot,echo=FALSE}
box_df <- as_tibble(brca.x.normal) %>%
  select(ends_with("_mean")) %>%
  rename_all(~ str_replace_all(., "_mean", "")) %>%
  mutate(brca["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")
p.mean <- ggplot(box_df,
       aes(factor(features,levels = measures),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c("#75c731", "#0488cf")) +
  labs(x = "Feature (mean)", y = "Standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7, angle = 45))
p.mean + plot_annotation(title = "Mean values distribution")
```
```{r se plot, echo=FALSE}
box_df <- as_tibble(brca.x.normal) %>%
  select(ends_with("_se")) %>%
  rename_all(~ str_replace_all(., "_se", "")) %>%
  mutate(brca["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")
p.mean <- ggplot(box_df,
       aes(factor(features,levels = measures),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c("#75c731", "#0488cf")) +
  labs(x = "Feature (standard error)", y = "Standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7, angle = 45))
p.mean + plot_annotation(title = "Standard error values distribution")
```
```{r worst plot, echo=FALSE}
box_df <- as_tibble(brca.x.normal) %>%
  select(ends_with("_worst")) %>%
  rename_all(~ str_replace_all(., "_worst", "")) %>%
  mutate(brca["diagnosis"]) %>%
  pivot_longer(col = -diagnosis, names_to = "features", values_to = "value")
p.mean <- ggplot(box_df,
       aes(factor(features,levels = measures),
           value, fill = diagnosis)) +
  geom_boxplot() +
  scale_fill_manual(values = c("#75c731", "#0488cf")) +
  labs(x = "Feature (worst)", y = "Standardized value") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(size = 7, angle = 45))
p.mean + plot_annotation(title = "Worst values distribution")
```
We see from the plots that the features are indeed related to the diagnosis, but that a single feature cannot predict the label alone.
Our challenge during this work will be to combine the features in a way that will give the best prediction.

# Feture selection and dimensionality reduction
Because the nature of our data, having 3 aspects of every feature, there might be biases while applying ML models using all the features, where there can be duplicates in the trends between them.
Moreover, the "worst" columns are often in correlation with the "mean" and the "se" of the same measure. [see supplement figure 1A]
In addition, even between the different measures, we can find duplicates:
The radius of a cell has strong correlation to it's perimeter and area [also in the supplement figure 1B].
To present our claim, here is the correlations between all the features:
```{r corr before pca, echo=FALSE}
corrs <- cor(brca.x.normal)
colnames(corrs) <- gsub("_", " ", colnames(corrs))
rownames(corrs) <- gsub("_", " ", rownames(corrs))
```
```{r}
corrplot(corrs, method = "square", tl.col = "black", tl.srt = 45, sig.level = 0.05, type = 'lower', diag = FALSE)
```
After the dimensionality reduction, we will see if the correlations between the features will be weaker, as expected.

To overcome the issues we mentioned, we:
1) preform PCA on all the aspects of the features "radius", "perimeter" and "area":
```{r pca shape, echo=FALSE}
mesures.shape <- c("radius", "perimeter", "area")
measures <- c("texture","smoothness", "compactness", "concavity", "concave_pts", "symmetry", "fractal_dim")
shape <- c()
for (prefix in mesures.shape) {
  # Find column names that start with the current prefix
  shape <- append(shape,paste0(prefix, "_mean"))
  shape <- append(shape,paste0(prefix, "_se"))
  shape <- append(shape,paste0(prefix, "_worst"))
}
brca.shape <- brca.x.normal[,shape]
pca <- prcomp(brca.shape)
summary(pca)
```
By taking PC1 and PC2 alone (instead all the 9 features) we can explain nearly 98% of the variance.
therefore, we will create 2 new features: "shape_pc1" and "shape_pc2" instead the previous ones.
```{r,echo=FALSE}
brca.reduce <- data.frame(brca[,c('diagnosis')])
colnames(brca.reduce) <- c("diagnosis")
brca.reduce$shape_pc1 <- pca$x[,1]
brca.reduce$shape_pc2 <- pca$x[,2]
```
2) preform PCA on every triplet of "mean", "standard error" and "worst".
we replace the original features with PC1 and PC2 if they can explain more than 95% of the variance.
```{r, echo=FALSE}
for (prefix in measures) {
  # Find column names that start with the current prefix
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")
  brca.subset <- brca.x.normal[,c(prefix.mean, prefix.se, prefix.worst)]
  cat("--------", prefix, "--------\n")
  pca <- prcomp(brca.subset)
  exp_pc2_var <- sum(pca$sdev[1:2]^2/sum(pca$sdev^2))
  if (exp_pc2_var < 0.95) {
    cat("PC2 Cumulative Proportion of Variance:", exp_pc2_var, "\nNot Preforming dimentionality reduction\n")
    brca.reduce[[paste0(prefix, "_mean")]] <- brca.x.normal[,prefix.mean]
    brca.reduce[[paste0(prefix, "_se")]] <- brca.x.normal[,prefix.se]
    brca.reduce[[paste0(prefix, "_worst")]] <- brca.x.normal[,prefix.worst]
  }
  else{
    cat(prefix, ": PC2 Cumulative Proportion of Variance:", exp_pc2_var, "\nreplace", prefix,"mean, se and worst to PC1 and PC2\n")
    brca.reduce[[paste0(prefix, "_pc1")]] <- pca$x[,1]
    brca.reduce[[paste0(prefix, "_pc2")]] <- pca$x[,2]
  }
}
brca.x.reduce <- brca.reduce[,!(colnames(brca.reduce) %in% c("diagnosis"))]
```
Now we check the correlation matrix between the features in the reduced data
```{r corr after pca,echo=FALSE}
corrs <- cor(brca.x.reduce)
colnames(corrs) <- gsub("_", " ", colnames(corrs))
rownames(corrs) <- gsub("_", " ", rownames(corrs))
```
```{r}
corrplot(corrs, method = "square", tl.col = "black", tl.srt = 45, sig.level = 0.05, type = 'lower', diag = FALSE)
```
During this analysis we will apply our methods over the raw data and the reduced version and examine if the reduced version will deliver better results.
*Important Note:* We also did PCA on the entire data set and examined the result on this df as well. The result were not better so we didnt include all the algorithms and analysis on this df.

To visually see the two data sets we plotted t-SNE on both sets, and colored the observations according to the diagnosis:
```{r,echo=FALSE}
reduce.tsne <- Rtsne(brca.x.reduce,
  pca = FALSE, perplexity = 10,
  theta = 0.0
)
reduce.tsne <- data.frame(
  TSNE1 = reduce.tsne$Y[, 1],
  TSNE2 = reduce.tsne$Y[, 2],
  Diagnosis = brca$diagnosis)
p1 <- ggplot(reduce.tsne, aes(
  x = TSNE1, y = TSNE2,
  col = Diagnosis
)) +
  geom_point() + ggtitle("After dimantionality reduction") + theme(legend.position = "none")
full.tsne <- Rtsne(brca.x,
  pca = FALSE, perplexity = 10,
  theta = 0.0
)
full.tsne <- data.frame(
  TSNE1 = full.tsne$Y[, 1],
  TSNE2 = full.tsne$Y[, 2],
  Diagnosis = brca$diagnosis)
p2 <- ggplot(full.tsne, aes(
  x = TSNE1, y = TSNE2,
  col = Diagnosis
)) +
  geom_point()  + ggtitle("Before dimantionality reduction")
grid.arrange(p2, p1, widths=c(0.55, 0.45), ncol=2)
```
We can see that after the feature selection + PCA there is a better seperation of the data points. 

### supplementary Plots:
correlation between triplets of "mean", "se" and "worst".
```{r corr triplets plot, echo=FALSE ,include=FALSE}
for (prefix in measures) {
  # Find column names that start with the current prefix
  prefix.mean <- paste0(prefix, "_mean")
  prefix.se <- paste0(prefix, "_se")
  prefix.worst <- paste0(prefix, "_worst")
  brca.subset <- brca.x.normal[,c(prefix.mean, prefix.se, prefix.worst)]
  colnames(brca.subset) <- gsub("_", " ", colnames(brca.subset))
  cor(brca.subset) %>% corrplot(method = "square", tl.col = "black", tl.srt = 45, sig.level = 0.05, type = 'lower', diag = FALSE)
}
```
correlations between all the features that related to the shape of the tumor cells (radius, perimeter and area)
```{r corr shape plot}
colnames(brca.shape) <- gsub("_", " ", colnames(brca.shape))
cor(brca.shape) %>%
corrplot(method = "square", tl.col = "black", tl.srt = 45,
         sig.level = 0.05, type = 'lower', diag = FALSE)
```

# Applying Different Machine Learning Algorithms On The Data:
First of all we split the data to train and test, we use the same split in all of the algorithms afterwards. 
We chose to use the following algorithms:
• KNN
• SVM
• Random Forest
• XGBOOST

SVM: Chosen for its ability to find complex relationships between features and classes through optimal hyperplane separation.
XGBoost: Selected for its strong performance in handling imbalanced datasets and capturing intricate feature interactions.
Random Forest: Picked for its ensemble approach, effectively reducing overfitting and providing feature importance scores.
KNN: Included for its straightforward methodology of assigning feature importance based on the characteristics of nearby data points.

# KNN Algorithm
First, we will check the KNN algorithm on the raw data (before performing the PCA):
```{r,echo=FALSE}
set.seed(613)
# Shuffle the data:
shuffled_data <- brca.normal[sample(nrow(brca.normal)), ]

# Create index to split based on labels  
index <- createDataPartition(shuffled_data$diagnosis, p=0.8, list=FALSE)

# Subset training set with index
brca.normal.training <- shuffled_data[index,]

# Subset test set with index
brca.normal.test <- shuffled_data[-index,]


# Train a model
model_knn <- train(brca.normal.training[, 2:31], brca.normal.training[, 1], method='knn')


# Access the optimal k value
best_k <- model_knn$bestTune$k
# Print the best k value
cat("Best k is:", best_k, "\n\n")

# Predict the labels of the test set
predictions<-predict.train(object=model_knn,brca.normal.test[,2:31], type="raw")


# Confusion matrix 


conf_matrix <- confusionMatrix(predictions,brca.normal.test[,1], positive="Malignant")
# Extract accuracy
accuracy_knn_n <- conf_matrix$overall["Accuracy"]

# Extract sensitivity (true positive rate)
sensitivity_knn_n <- conf_matrix$byClass["Sensitivity"]
# Print the confusion matrix
print(conf_matrix$table)
cat("\n")
# Print accuracy and sensitivity
cat("Accuracy:", accuracy_knn_n, "\n")
cat("Sensitivity:", sensitivity_knn_n, "\n")
```

Now we will check the knn algorithm on the reduced data:
```{r, echo=FALSE}
set.seed(613)
# Shuffle the data:
shuffled_data <- brca.reduce[sample(nrow(brca.reduce)), ]

# Create index to split based on labels  
index <- createDataPartition(shuffled_data$diagnosis, p=0.8, list=FALSE)

# Subset training set with index
brca.reduced.training <- shuffled_data[index,]

# Subset test set with index
brca.reduced.test <- shuffled_data[-index,]



# Train a model
model_knn <- train(brca.reduced.training[, 2:20], brca.reduced.training[, 1], method='knn')


# Access the optimal k value
best_k <- model_knn$bestTune$k
# Print the best k value
cat("Best k is:", best_k, "\n\n")

# Predict the labels of the test set
predictions<-predict.train(object=model_knn,brca.reduced.test[,2:20], type="raw")


# Confusion matrix 
conf_matrix<-confusionMatrix(predictions,brca.reduced.test[,1], positive="Malignant")
# Extract accuracy
accuracy_knn_r <- conf_matrix$overall["Accuracy"]

# Extract sensitivity (true positive rate)
sensitivity_knn_r <- conf_matrix$byClass["Sensitivity"]

# Print the confusion matrix
print(conf_matrix$table)
cat("\n")
# Print accuracy and sensitivity
cat("Accuracy:", accuracy_knn_r, "\n")
cat("Sensitivity:", sensitivity_knn_r, "\n")

```
As we can see, the accuracy of the KNN model improved only by a bit after the dimensionality reduction.
The sensitivity measurement, which indicates on the rate of the un-detected malignancy cases (high sensitivity = low un-detected rate).
Because the nature of our data, it is extremely important to get the highest sensitivity possible.
We will proceed to more algorithms in the pursuit after better sensitivity.

# SVM algorithm
We will perform SVM, which has the advantage of the ability to extract coefficients, and give us the importance of each feature.

We used the default hyper parameters, since they gave us the best results. We split the data into train set
(450 samples, 80%) and test set (119 samples, 20%)
Trying SVM on the raw data:
```{r,echo=FALSE}
set.seed(23)
# Subset training set with index
train_sample <- sample(569, 456)
brca.normal.training<- brca.x.normal[train_sample, ]
brca.normal.training.labels <- as.factor(brca.normal$diagnosis[train_sample])
brca.normal.test <- brca.x.normal[-train_sample, ]
brca.normal.test.labels <- as.factor(brca.normal$diagnosis[-train_sample])

# Train the SVM model
regressor_svm <- svm(formula = brca.normal.training.labels ~ .,
                     data = brca.normal.training,
                     type = 'C-classification',
                     kernel = 'linear')

# Make predictions on the test set
y_pred1 <- predict(regressor_svm, newdata = brca.normal.test)


# Confusion matrix 
conf_matrix<-confusionMatrix(data=y_pred1, reference=brca.normal.test.labels, positive="Malignant")

# Extract accuracy
accuracy_svm_n <- conf_matrix$overall["Accuracy"]

# Extract sensitivity (true positive rate)
sensitivity_svm_n <- conf_matrix$byClass["Sensitivity"]

# Print the confusion matrix
print(conf_matrix$table)
cat("\n")
# Print accuracy and sensitivity
cat("Accuracy:", accuracy_svm_n, "\n")
cat("Sensitivity:", sensitivity_svm_n, "\n")

```
```{r,echo=FALSE}
# Extract SVM coefficients and SVs
SVM_coefs <- t(regressor_svm$coefs) %*% regressor_svm$SV

# Convert SVM coefficients to a data frame for easier plotting
coefs_df <- data.frame(factors = colnames(SVM_coefs), importance = t(SVM_coefs))
coefs_df$importance <- abs(coefs_df$importance)
# Sort the factors by their importance
top_inds <- order(coefs_df$importance)
coefs_df <- coefs_df[top_inds, ]

# Create a bar plot of factors' importance with colors and rotated labels
barplot(coefs_df$importance, names.arg = coefs_df$factors, 
        main = "SVM Factors Importance", ylab = "Importance (abs)",
        col =  "purple", las = 2, cex.names = 0.55)
```

Trying the SVM on the reduced data:
```{r,echo=FAlSE}
set.seed(23)

# Subset training set with index
train_sample <- sample(569, 456)
brca.x.reduce.training<- brca.x.reduce[train_sample, ]
brca.x.reduce.training.labels <- as.factor(brca.reduce$diagnosis[train_sample])
brca.x.reduce.test <-brca.x.reduce[-train_sample, ]
brca.x.reduce.test.labels <- as.factor(brca.reduce$diagnosis[-train_sample])


# Train the SVM model
regressor_svm <- svm(formula = brca.x.reduce.training.labels ~ .,
                     data = brca.x.reduce.training,
                     type = 'C-classification',
                     kernel = 'linear')

# Make predictions on the test set
y_pred1 <- predict(regressor_svm, newdata = brca.x.reduce.test)

# Compute confusion matrix
conf_matrix <- confusionMatrix(data = y_pred1, reference = brca.x.reduce.test.labels, positive = "Malignant")


# Extract accuracy
accuracy_svm_r <- conf_matrix$overall["Accuracy"]

# Extract sensitivity (true positive rate)
sensitivity_svm_r <- conf_matrix$byClass["Sensitivity"]

# Print the confusion matrix
print(conf_matrix$table)
cat("\n")
# Print accuracy and sensitivity
cat("Accuracy:", accuracy_svm_r, "\n")
cat("Sensitivity:", sensitivity_svm_r, "\n")


```

```{r,echo=FALSE}
# Extract SVM coefficients and SVs
SVM_coefs <- t(regressor_svm$coefs) %*% regressor_svm$SV

# Convert SVM coefficients to a data frame for easier plotting
coefs_df <- data.frame(factors = colnames(SVM_coefs), importance = t(SVM_coefs))

coefs_df$importance <- abs(coefs_df$importance)
# Sort the factors by their importance
top_inds <- order(coefs_df$importance)
coefs_df <- coefs_df[top_inds, ]



# Define colors for the bar plot

# Create a bar plot of factors' importance with colors and rotated labels
barplot(coefs_df$importance, names.arg = coefs_df$factors, 
        main = "SVM Factors Importance", ylab = "Importance (abs)",
        col = "purple", las = 2, cex.names = 0.55)

```
Like in the KNN, the dimensionality reduction improved the results just a little.
Compered to the KNN, SVM gave us better results, and we also got feature importance ranking:
In both raw data and reduced data we can see that the most important features for the prediction are the shape features (radius and area), the concave points, and concavity.


#Random Forest Algorithm
We applied the Random Forest algorithm on the two data sets. 
We did a grid search to find the best Number of trees (50/100/150) and tree depth (5/10/15)
First we apply the algorithm on the data set without the feature reduction. 
```{r, echo=FALSE}
set.seed(123)

# Take 80 percent for training and shuffle the data.
train_indices <- sample(nrow(brca.normal), nrow(brca.normal) * 0.8)
train_data <- brca.normal[train_indices, ]

# Assign the remaining data to the test set.
test_data <- brca.normal[-train_indices, ]

# Exclude the first column for training features.
train_features_normal <- train_data[, -1]

# Keep just the first column as the training target.
train_target_normal <- train_data[, 1]

# Exclude the first column for test features.
test_features_normal <- test_data[, -1]

# Keep just the first column as the test target.
test_target_normal <- test_data[, 1]

```

```{r, echo=FALSE}
set.seed(123)

# Train the random forest models with different parameters
num_trees <- c(50, 100, 150)  # Different numbers of trees
max_depth <- c(5, 10, 15)  # Different depths
results <- list()


for (n in num_trees) {
  for (d in max_depth) {
    model <- randomForest(train_features_normal, train_target_normal, ntree = n, maxdepth = d)
    predictions <- predict(model, test_features_normal)
    accuracy_normal_rf <- sum(predictions == test_target_normal) / length(test_target_normal)
    
    # Calculate confusion matrix
    confusion <- confusionMatrix(predictions, test_target_normal, positive = "Malignant")
    sensitivity_normal_rf <- confusion$byClass["Sensitivity"]
    

    result <- list(ntrees = n, maxdepth = d, model = model, predictions = predictions, accuracy = accuracy_normal_rf, sensitivity = sensitivity_normal_rf)
    results <- c(results, list(result))
  }
}

# Find the model with the highest accuracy
best_result <- results[[which.max(sapply(results, function(x) x$accuracy))]]
best_model_normal <- best_result$model
best_predictions <- best_result$predictions

best_sensitivity_normal_rf <- best_result$sensitivity
```
From all the trained trees we choose the one with the highest accuracy and plotted the Gini Importance score for each feature. 
```{r, echo=FALSE}
# Get feature importance data using gini importance
importance <- importance(best_model_normal, type=2)
importance_df <- data.frame(Feature = row.names(importance), Importance = importance[, 1])
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot feature importance
importance_plot <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

plot(importance_plot)

```
```{r, fig.width=10}

# Get the data used to fit the best Random Forest model
tree_data <- data.frame(train_features_normal, target = train_target_normal)

# Fit a single conditional inference tree (ctree) using the same parameters as your best Random Forest tree
# (you'll need to adjust this to match the best tree's parameters)
ctree_model <- ctree(target ~ ., data = tree_data, controls = ctree_control(maxdepth = best_result$maxdepth))

# Plot the tree with specific spacing parameters
plot(ctree_model, 
     terminal_panel = node_terminal(ctree_model, 
                                    abbreviate = TRUE, 
                                    digits = 2, 
                                    fill = "lightgray"),
     tnex = 1) # You can adjust this parameter to control the spacing between nodes

```

After that we did the same process on the reduced data. 
```{r, echo=FALSE}
set.seed(123)

# Take 80 percent for training and shuffle the data.
train_indices <- sample(nrow(brca.reduce), nrow(brca.reduce) * 0.8)
train_data <- brca.reduce[train_indices, ]

# Assign the remaining data to the test set.
test_data <- brca.reduce[-train_indices, ]

# Exclude the first column for training features.
train_features_reduce <- train_data[, -1]

# Keep just the first column as the training target.
train_target_reduce <- train_data[, 1]

# Exclude the first column for test features.
test_features_reduce <- test_data[, -1]

# Keep just the first column as the test target.
test_target_reduce <- test_data[, 1]
```

```{r, echo=FALSE}
set.seed(123)

# Train the random forest models with different parameters for brca.reduce
num_trees_reduce <- c(50, 100, 150)  # Different numbers of trees
max_depth_reduce <- c(5, 10, 15)  # Different depths
results_reduce <- list()

for (n in num_trees_reduce) {
  for (d in max_depth_reduce) {
    model_reduce <- randomForest(train_features_reduce, train_target_reduce, ntree = n, maxdepth = d)
    predictions_reduce <- predict(model_reduce, test_features_reduce)
    accuracy_reduce_rf <- sum(predictions_reduce == test_target_reduce) / length(test_target_reduce)
    
    confusion <- confusionMatrix(predictions_reduce, test_target_reduce, positive = "Malignant")
    sensitivity_reduce_rf <- confusion$byClass["Sensitivity"]
    
    result_reduce <- list(ntrees = n, maxdepth = d, model = model_reduce, predictions = predictions_reduce, accuracy = accuracy_reduce_rf ,sensitivity = sensitivity_reduce_rf)
    results_reduce <- c(results_reduce, list(result_reduce))
  }
}

# Find the model with the highest accuracy for brca.reduce
best_result_reduce <- results_reduce[[which.max(sapply(results_reduce, function(x) x$accuracy))]]
best_model_reduce <- best_result_reduce$model
best_predictions_reduce <- best_result_reduce$predictions

best_sensitivity_reduce_rf <- best_result_reduce$sensitivity
```
 The following plot represent the Gini Importance Score for each feature on the reduced data set. 
```{r, echo=FALSE}
# Get feature importance data using gini importance for brca.reduce
importance_reduce <- importance(best_model_reduce, type=2)
importance_df_reduce <- data.frame(Feature = row.names(importance_reduce), Importance = importance_reduce[, 1])
importance_df_reduce <- importance_df_reduce[order(importance_df_reduce$Importance, decreasing = TRUE), ]

# Plot feature importance for brca.reduce
importance_plot_reduce <- ggplot(importance_df_reduce, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

plot(importance_plot_reduce)
```
We can see that after the dimensionality reduction + Feature selection we were able to find a feature that contribute to the classification much better than in the original data set. 
<ADD EXAMPLES>

Now we comapre the accuracy of the 2 models that were built on two different data sets. 
```{r}
# Compare the accuracies
performance <- data.frame(Dataset = c("brca.normal", "brca.reduce"),
                          Accuracy = c(accuracy_normal_rf, accuracy_reduce_rf))
performance

```
We can see that the accuracy of the model that were trained on the data set after feature selection and PCA were higher. 

<ADD MORE SCORING SUCH AS SENSTIVITY> 

#XGBOOST
We now apply xg boost algorithm on the two data sets.
```{r, echo=FALSE,results='hide'}
# Convert labels to numeric values
train_target_numeric <- ifelse(train_target_normal == "Benign", 0, 1)
test_target_numeric <- ifelse(test_target_normal == "Benign", 0, 1)

# Train the xgboost model
xgb_model_normal <- xgboost(data = as.matrix(train_features_normal), label = train_target_numeric, nrounds = 200, objective = "binary:logistic")

# Make predictions on the test set
predictions <- predict(xgb_model_normal, as.matrix(test_features_normal))

# Convert predictions to class labels
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)

# Calculate accuracy
accuracy_normal <- sum(predicted_labels == test_target_numeric) / length(test_target_numeric)

# Calculate precision
precision <- sum(predicted_labels == 1 & test_target_numeric == 1) / sum(predicted_labels == 1)

# Calculate recall
recall <- sum(predicted_labels == 1 & test_target_numeric == 1) / sum(test_target_numeric == 1)
sensitivity_xg_normal <- recall 

# Calculate F1-score
f1_score <- 2 * precision * recall / (precision + recall)

# Calculate AUC-ROC
roc <- roc(test_target_numeric, predictions)
auc_roc <- auc(roc)
```

```{r}
# Print performance metrics
cat("Performance Metrics for brca.normal dataset:\n")
cat("Accuracy:", accuracy_normal, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
cat("AUC-ROC:", auc_roc, "\n")
```
We were able to see that logloss started to converge after 158 rounds. 

Feature importance for the normal data model 
```{r}
# Get feature importance matrix
importance_matrix <- xgb.importance(model = xgb_model_normal)

# Convert importance matrix to data.frame
importance_df <- data.frame(Feature = importance_matrix$Feature, Importance = importance_matrix$Gain)

# Sort the importance in descending order
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot feature importance
importance_plot <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

print(importance_plot)

```
Now we did the same process on the reduced data set. 
```{r,echo=FALSE,results='hide'}

set.seed(123)
# Convert labels to numeric values
train_target_numeric <- ifelse(train_target_reduce == "Benign", 0, 1)
test_target_numeric <- ifelse(test_target_reduce == "Benign", 0, 1)

# Train the XGBoost model on brca.reduce
xgb_model_reduce <- xgboost(data = as.matrix(train_features_reduce), label = train_target_numeric, nrounds = 300, objective = "binary:logistic")

# Make predictions on test set
predictions_reduce <- predict(xgb_model_reduce, as.matrix(test_features_reduce))

# Convert predictions to class labels
predicted_labels_reduce <- ifelse(predictions_reduce >= 0.5, 1, 0) # Updated variable name

# Calculate accuracy
accuracy_reduce <- sum(predicted_labels_reduce == test_target_numeric) / length(test_target_numeric)

# Calculate precision
precision <- sum(predicted_labels_reduce == 1 & test_target_numeric == 1) / sum(predicted_labels_reduce == 1)

# Calculate recall
recall <- sum(predicted_labels_reduce == 1 & test_target_numeric == 1) / sum(test_target_numeric == 1)
sensitivity_xg_reduce <- recall

# Calculate F1-score
f1_score <- 2 * precision * recall / (precision + recall)

# Calculate AUC-ROC
roc <- roc(test_target_numeric, predictions_reduce) # Updated variable name
auc_roc <- auc(roc)


```

For example here we show a specific decision tree that was made during the XGBOOST algorithm. 
```{r}
xgb.plot.tree(model = xgb_model_reduce, trees = 2, show_node_id = TRUE)
```
It's essential to note that the tree being shown here is only one of many in the ensemble. The final prediction is a combined result of all the trees, and the one being visualized may not fully represent the complexity and behavior of the overall model.

Each tree in XGBOOST have different emphasis in classifying the data. We can see in this tree that the features shape_pc1 and texture_pc1 which was shown before as important for the classification, is used multiple time in different branches with different thresholds. This shows its important in classifying the data and signifying that it might have a non-linear relationship with the target variable.
Also we can see that concave_pts_pc  appears as the first split, meaning that it has the highest importance in distinguishing between the classes.

```{r}
# Print performance metrics
cat("Performance Metrics for brca.normal dataset:\n")
cat("Accuracy:", accuracy_reduce, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
cat("AUC-ROC:", auc_roc, "\n")
```

Feature importance for the reudced features model 
```{r}
# Get feature importance matrix
importance_matrix <- xgb.importance(model = xgb_model_reduce)

# Convert importance matrix to data.frame
importance_df <- data.frame(Feature = importance_matrix$Feature, Importance = importance_matrix$Gain)

# Sort the importance in descending order
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Plot feature importance
importance_plot <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(x = "Feature", y = "Importance") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

print(importance_plot)
```
We can see that in both of the model the most significant feature for classification is the area (shape?) and the concave. 
```{r}
# Compare the accuracies
performance_xgb <- data.frame(Dataset = c("brca.normal", "brca.reduce"),
                              Accuracy = c(accuracy_normal, accuracy_reduce))
performance_xgb

```
Also here we can see that the performance of the reduces data set is better.

<ADD perfomance comparison of all models> 
```{r}
# Compare the accuracies
performance_ <- data.frame(Dataset = c("brca.normal_accuracy", "brca.reduce_accuracy","brca.normal_sensitivity", "brca.reduce_sensitivity"),
                              KNN = c(accuracy_knn_n,accuracy_knn_r,sensitivity_knn_n,sensitivity_knn_r),
                              SVM = c(accuracy_svm_n,accuracy_svm_r,sensitivity_svm_n,sensitivity_svm_r),
                              Random_Forest = c(accuracy_normal_rf, accuracy_reduce_rf ,best_sensitivity_normal_rf,best_sensitivity_reduce_rf),
                              XGBOOST = c(accuracy_normal, accuracy_reduce,sensitivity_xg_normal,sensitivity_xg_reduce))
performance_
```

#Summary 
As shown in the above table, we can see that the PCA we did improved the model's performances in all algorithms.
The algorithm with the best accuracy is the SVM but the model that has the best

